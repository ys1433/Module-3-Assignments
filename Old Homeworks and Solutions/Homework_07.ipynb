{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 07: Ensemble Methods – Bagging, Random Forests, and Gradient Boosting\n",
    "\n",
    "## Due: Midnight on March 16 (with 2-hour grace period) and worth 75 points\n",
    "\n",
    "Over the past two weeks, we have expanded our machine learning toolkit by moving beyond linear regression to explore decision trees, which introduced a variety of interacting parameters. Through Homework 6, you developed a systematic workflow for parameter tuning that balances manual exploration and automated searches (e.g., random or grid search) to optimize performance while gaining insights into model behavior.\n",
    "\n",
    "This week, we take another step forward by studying ensemble methods which combine multiple decision trees to produce even stronger predictive models. Specifically, we will investigate:\n",
    "\n",
    "- Bagging  \n",
    "- Random Forests  \n",
    "- Gradient Boosting  \n",
    "\n",
    "These three approaches build on the decision tree we studied last week. Our  workflow from Homework 6 will again serve as the foundation for optimizing these more complex models.\n",
    "\n",
    "### What We Will Do in This Homework\n",
    "\n",
    "To analyze and optimize our ensemble models, we will apply the two-phase strategy introduced in Homework 6 (but with grid search rather than random grid search):\n",
    "\n",
    "1. **First Phase:**  \n",
    "   - Iteratively sweep through key parameters in coarse ranges  \n",
    "   - Visualize training, validation, and test MSE  \n",
    "   - Diagnose overfitting or underfitting  \n",
    "   - Examine the standard deviation of CV scores to understand model stability (review Problems 1 & 2 in Homework 6)\n",
    "\n",
    "2. **Second Phase:**  \n",
    "   - Focus on the most unstable or promising parameter ranges found in Phase 1  \n",
    "   - Perform an exhaustive search within these narrower ranges using `GridSearchCV` (review Problem 4 in Homework 6)\n",
    "\n",
    "Refer to **Appendix 4** for more details.\n",
    "\n",
    "We will follow this process for each of the three ensemble methods, systematically tuning their four most important parameters:\n",
    "\n",
    "1. **Bagging Regressor:** `n_estimators`, `max_samples`, `max_features`, `bootstrap`  \n",
    "2. **Random Forest Regressor:** `n_estimators`, `max_depth`, `max_features`, `bootstrap`  \n",
    "3. **Gradient Boosting Regressor:** `learning_rate`, `n_estimators`, `max_depth`, `max_features`\n",
    "\n",
    "Throughout this homework, we will continue using `RepeatedKFold` cross-validation to reduce variance in our CV MSE estimates. The default number of repetitions is 10, but you may find it necessary to reduce this when initially searching broad parameter spaces—then increase it for fine-tuning as you zero-in on the best models. \n",
    "\n",
    "As before, you will store your best parameter values (and the resulting CV MSE) in a dictionary in order to track improvements across experiments and maintain a clear record of how each parameter choice was made.\n",
    "\n",
    "Before starting:\n",
    "- Review lesson materials on ensemble methods and watch the three videos on Bagging, Random Forests, and Gradient Boosting.  \n",
    "- Study the relevant appendices.  \n",
    "- Pay special attention to the Gradient Boosting video and notebook, as the code in this homework builds on (and has been adapted from) those resources.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This homework consists of 15 graded problems, each worth 5 points, for a total of 75 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold,GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from tqdm                    import tqdm\n",
    "\n",
    "import matplotlib.ticker as mticker           # Optional: you can print out y axis labels as dollars. \n",
    "\n",
    "# globals\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# utility code\n",
    "\n",
    "# Completely optional:  Format y-axis labels as dollars with commas\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ames Housing Dataset  \n",
    "\n",
    "The code cell below will load the dataset for you.  This is the same dataset we used for the last two homeworks. \n",
    "\n",
    "> **Notice** that this code includes a useful optimization: **before downloading, it first\n",
    "checks whether the files already exist.** This is a essential step when working with large datasets or when building deep learning models, where training can span hours or even days. By reusing previously downloaded files or saved models, you can avoid unnecessary work and significantly speed up your workflow.\n",
    "\n",
    "For a detailed description of the dataset features, please refer to the **Appendix** in Homework 05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files already exist. Skipping download.\n",
      "Training and testing datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Ames_Dataset\"                              # Directory where files will be stored\n",
    "\n",
    "# Check if one of the files exists; if not, download and extract the zip file\n",
    "\n",
    "if not os.path.exists( os.path.join(data_dir, \"X_train.csv\") ):\n",
    "    print(\"Dataset files not found. Downloading...\")\n",
    "    zip_url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/ames_housing.zip\"\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        # Extract the zip file into the designated directory\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "        print(\"Files downloaded and extracted successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(\"Dataset files already exist. Skipping download.\")\n",
    "\n",
    "# Load the datasets\n",
    "X_train = pd.read_csv(os.path.join(data_dir, \"X_train.csv\"))\n",
    "X_test  = pd.read_csv(os.path.join(data_dir, \"X_test.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(data_dir, \"y_train.csv\")).squeeze(\"columns\")    \n",
    "y_test  = pd.read_csv(os.path.join(data_dir, \"y_test.csv\")).squeeze(\"columns\")\n",
    "\n",
    "print(\"Training and testing datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Wrapper Functions for Running Ensemble Models\n",
    "\n",
    "The following cells are adapted from the Week 7 video notebook on `GradientBoostingRegressor`, but have been refactored so that they can handle any of the three ensemble models (Bagging, Random Forest, or Gradient Boosting). Key changes include:\n",
    "\n",
    "- **`run_model`** replaces the original `run_gradient_boosting_regressor` and accepts a parameter dictionary that can be applied to any of the three ensemble models.  \n",
    "- **`sweep_parameter`** is updated to work seamlessly with `run_model`, letting you:\n",
    "  - Specify which ensemble model you want to use.  \n",
    "  - Pass a dictionary of model parameters.  \n",
    "  - Return a modified parameter dictionary reflecting the best value of the parameter you swept, along with the corresponding MSE  (note that RMSE is **only** used for printing out results)\n",
    "\n",
    "> **Note:** Please do not change these cells unless you consult with the LFs first. Any alterations may cause downstream issues with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model, X_train, y_train, X_test, y_test, n_repeats=10, n_jobs=-1, **model_params):\n",
    "\n",
    "    # Instantiate the model if a class is provided, so for example can use either BaggingRegressor or BaggingRegressor() as argument. \n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "\n",
    "    neg_mse_scores = cross_val_score(model, X_train, y_train,scoring = 'neg_mean_squared_error',\n",
    "                                     cv = RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=42), n_jobs  = n_jobs)\n",
    "    \n",
    "    mean_cv_mse = -np.mean(neg_mse_scores)\n",
    "    std_cv_mse  = np.std(neg_mse_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training MSE and testing MSE\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_mse   = mean_squared_error(y_train, train_preds)\n",
    "    test_preds  = model.predict(X_test)\n",
    "    test_mse    = mean_squared_error(y_test, test_preds)\n",
    "    \n",
    "    return mean_cv_mse, std_cv_mse, train_mse, test_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_parameter(model,\n",
    "                    Parameters,\n",
    "                    param,\n",
    "                    parameter_list,\n",
    "                    X_train          = X_train,\n",
    "                    y_train          = y_train,\n",
    "                    X_test           = X_test,\n",
    "                    y_test           = y_test,\n",
    "                    verbose          = True,\n",
    "                    show_rmse        = True,\n",
    "                    n_iter_no_change = None,\n",
    "                    delta            = 0.001,\n",
    "                    n_jobs           = -1,\n",
    "                    n_repeats        = 10):\n",
    "    \n",
    "    start = time.time()\n",
    "    Parameters = Parameters.copy()  # Avoid modifying the original dictionary\n",
    "    \n",
    "    cv_mses, std_cvs, train_mses, test_mses = [], [], [], []\n",
    "    no_improve_count = 0\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "    # Run over each value in parameter_list\n",
    "    for p in tqdm(parameter_list, desc=f\"Sweeping {param}\"):\n",
    "        Parameters[param] = p\n",
    "        P_temp = Parameters.copy()\n",
    "        # Remove MSE_found if present, just in case\n",
    "        P_temp.pop('MSE_found', None)\n",
    "        \n",
    "        cv_mse, std_cv, train_mse, test_mse = run_model(\n",
    "            model=model,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_test=X_test,   y_test=y_test,\n",
    "            n_repeats=n_repeats,\n",
    "            n_jobs=n_jobs,\n",
    "            **P_temp\n",
    "        )\n",
    "        cv_mses.append(cv_mse)\n",
    "        std_cvs.append(std_cv)\n",
    "        train_mses.append(train_mse)\n",
    "        test_mses.append(test_mse)\n",
    "        \n",
    "        # Early-stopping logic\n",
    "        if cv_mse < best_mse - delta:\n",
    "            best_mse = cv_mse\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if n_iter_no_change is not None and no_improve_count >= n_iter_no_change:\n",
    "            print(f\"Early stopping: No improvement after {n_iter_no_change} iterations.\")\n",
    "            break\n",
    "    \n",
    "    # Identify best parameter\n",
    "    min_cv_mse = min(cv_mses)\n",
    "    min_index = cv_mses.index(min_cv_mse)\n",
    "    best_param = parameter_list[min_index]\n",
    "    Parameters[param] = best_param\n",
    "    Parameters['MSE_found'] = min_cv_mse\n",
    "    \n",
    "    if verbose:\n",
    "        # Prepare for plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "        \n",
    "        # We only need as many parameter values as we actually computed\n",
    "        partial_param_list = parameter_list[:len(cv_mses)]\n",
    "        \n",
    "        # Check if our parameter list is Boolean so we can label accordingly\n",
    "        is_boolean = all(isinstance(val, bool) for val in partial_param_list)\n",
    "        if is_boolean:\n",
    "            # Convert booleans to integer indices for plotting\n",
    "            x_vals = list(range(len(partial_param_list)))\n",
    "            x_labels = [str(val) for val in partial_param_list]\n",
    "        else:\n",
    "            # Treat numeric or other types as-is\n",
    "            x_vals = partial_param_list\n",
    "            x_labels = partial_param_list\n",
    "        \n",
    "        error_name = 'RMSE' if show_rmse else 'MSE'\n",
    "        \n",
    "        # ----- First plot: (R)MSE -----\n",
    "        ax1.set_title(f\"{error_name} vs {param}\")\n",
    "        \n",
    "        # Apply dollar formatting ONLY if we're showing RMSE\n",
    "        if show_rmse:\n",
    "            ax1.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
    "        \n",
    "        # Plot lines\n",
    "        ax1.plot(x_vals,\n",
    "                 np.sqrt(cv_mses) if show_rmse else cv_mses,\n",
    "                 marker='.', label=f\"CV {error_name}\", color='blue')\n",
    "        ax1.plot(x_vals,\n",
    "                 np.sqrt(train_mses) if show_rmse else train_mses,\n",
    "                 marker='.', label=f\"Train {error_name}\", color='green')\n",
    "        ax1.plot(x_vals,\n",
    "                 np.sqrt(test_mses) if show_rmse else test_mses,\n",
    "                 linestyle='--', label=f\"Test {error_name}\", color='orange')\n",
    "        ax1.scatter([x_vals[min_index]],\n",
    "                    [np.sqrt(min_cv_mse) if show_rmse else min_cv_mse],\n",
    "                    marker='x', label=f\"Best CV {error_name}\", color='red')\n",
    "        \n",
    "        ax1.set_ylabel(error_name)\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        \n",
    "        # ----- Second plot: CV Std Dev -----\n",
    "        ax2.set_title(f\"CV Standard Deviation vs {param}\")\n",
    "        ax2.plot(x_vals, std_cvs, marker='.', label=f\"CV {error_name} Std\", color='blue')\n",
    "        ax2.set_xlabel(param)\n",
    "        ax2.set_ylabel(\"Standard Deviation\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.5)\n",
    "        \n",
    "        # If we are using boolean x-values, set custom ticks\n",
    "        if is_boolean:\n",
    "            ax2.set_xticks(x_vals)\n",
    "            ax2.set_xticklabels(x_labels)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end - start)))\n",
    "    \n",
    "    return Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem One: Bagging Trees**  \n",
    "\n",
    "In this problem, you will follow a structured workflow to tune and evaluate a **Bagging Regressor**. The process builds on the approach from the last two homeworks; note that, as Homework 06, we will apply **grid search** to systematically explore parameter values.  Note that we will **not** be used Early Stopping in this homework; this will be a valuable technique going forward, but in this homework we are still learning how these complex models perform.\n",
    "\n",
    "At the end of the analysis, you will not simply select the model with the lowest CV MSE score. Instead, you will critically examine the plots to determine whether another model configuration provides a better balance between performance, stability, and generalization. This step is essential for ensuring that your chosen model will perform well on new data. The criteria we will use for this choice are in **Appendix 5** below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1.A: Iteratively Sweep Parameters and Visualize Results using `sweep_parameter(...)`**  \n",
    "We will start with the `Default_Parameters_Bagging` dictionary and iteratively adjust key parameters. At each step, using the code provided above, you will:  \n",
    "\n",
    "- Test a range of values for the specified parameter.\n",
    "- Plot training, repeated cross-validation, and test MSE to diagnose overfitting or underfitting.\n",
    "- Plot the standard deviation of the CV scores to assess model stability.\n",
    "- Store the best value (which produces the minimum CV MSE) in a dictionary, perhaps called `Parameters_BT`.\n",
    "\n",
    "**You should read Appendix 4 now if you have not done so already.**\n",
    "\n",
    "**Step-by-step process:**\n",
    "1. **Sweep `n_estimators`** (integer values):  \n",
    "   - Begin by *making a copy* of the provided `Default_Parameters_Bagging` dictionary.  \n",
    "   - Sweep a range of **integer** values for `n_estimators` (the number of base learners).  \n",
    "   - Store the best value in `Parameters_BT`. \n",
    "\n",
    "2. **Sweep `max_samples`** (float values):  \n",
    "   - Using `Parameters_BT`, test a range of **float** values for `max_samples`.  \n",
    "   - Store the best value in `Parameters_BT`. (*Note: This may simply be the default value!*)\n",
    "\n",
    "3. **Sweep `max_features`** (integer values):  \n",
    "   - Using `Parameters_BT`, test a range of **integer** values for `max_features`.  \n",
    "   - Store the best value in `Parameters_BT`. (*Again, the best value might be the default!*)\n",
    "\n",
    "4. **Sweep `bootstrap`** (Boolean values):  \n",
    "   - Test both possible values (`True` and `False`).  \n",
    "   - Store the best value in `Parameters_BT`.\n",
    "\n",
    "5. **Report your final results (including all final parameter choices, and convert MSE to RMSE in dollars when printing out)** and answer the graded question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the BaggingRegressor\n",
    "Default_Parameters_Bagging = {\n",
    "    'n_estimators': 10,            # Number of base estimators in the ensemble\n",
    "    'max_samples' : 1.0,           # Fraction of samples to draw for each base estimator\n",
    "    'max_features': 1.0,           # Fraction of features to consider for each estimator\n",
    "    'bootstrap'   : True,          # Use bootstrap samples when building estimators\n",
    "    'random_state': 42,            # Ensures reproducibility\n",
    "    'MSE_found'   : float('inf')   # Used for tracking the best MSE during parameter sweeps, to record result of these parameter choices\n",
    "                                   # Initialized to inf in case want to use this to record best MSE found so far in a sequence of parameter choices\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.A Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1a = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part A\n",
    "\n",
    "a1a = 0.0                                # Just to get it to run without error; your answer here (remember to use the RMSE)         \n",
    "\n",
    "print(f'a1a = ${a1a:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1.B: Refine Parameters for Model Stability**  \n",
    "After completing the first sweep, **repeat Part A as needed** to refine your model.  \n",
    "- Your goal is to identify the parameter combination that minimizes the **CV MSE**.  \n",
    "- **Final tuning goals:**\n",
    "  - Adjust `n_estimators` with a **tolerance of 10**.\n",
    "  - Adjust `max_samples` with a **tolerance of 0.1**.\n",
    "  - Adjust `max_features` with a **tolerance of 1**.\n",
    "- **Report your final results (including all final parameter choices, and convert MSE to RMSE in dollars when printing out)** and answer the graded question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.B Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part B (it may not be different than Part A)\n",
    "\n",
    "a1b = 0.0                                # Just to get it to run without error; your answer here (remember to use the RMSE)          \n",
    "\n",
    "print(f'a1b = ${a1b:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1.C: Use `GridSearchCV` for Exhaustive Search**  \n",
    "Once you have completed your parameter sweeps in Part B, you will verify and perhaps even refine your results using exhaustive grid search (not random search). \n",
    "- In your results from Part B, identify **unstable parameter ranges** (review Problem 4 from Homework 06).\n",
    "- Perform  **exhaustive grid searches** within appropriately restricted ranges using `GridSearchCV`.  \n",
    "- **Print out the best result found by exhaustive search (including the final parameter choices, and convert MSE to RMSE in dollars)** and answer the graded question.\n",
    "\n",
    "NOTE: Do *not* simply redo all of Parts A and B, and you should repeat your grid search runs using the techniques described in Appendix 4 until you are sure you have either verified your results from Part B or found a model with an even lower CV MSE score than you found in Part B.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.C Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1c = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found in Part C (it may not be different than Parts A and B)\n",
    "\n",
    "a1c = 0.0                                # Just to get it to run without error; your answer here (remember to use the RMSE)           \n",
    "\n",
    "print(f'a1c = ${a1c:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1.D: Evaluate Model Generalization**  \n",
    "At this point, you *may* have **two competing models**:  \n",
    "1. The model found in **Part B** (from parameter sweeps).  \n",
    "2. The model found in **Part C** (from `GridSearchCV`).  \n",
    "\n",
    "To determine which model will **generalize best to new data**, **read Appendix 5** and carefully consider:\n",
    "1. Does the **Part B model** perform best?\n",
    "2. Does the **Part C model** (if different) generalize better?\n",
    "3. Are there **additional insights from the plots** that suggest an alternative choice?  \n",
    "\n",
    "If your decision is (3), choose the parameters by examination of the plots to build your best model for **Part E.** \n",
    "There is no precise algorithm for choosing a models by examination of plots, and this is the kind of \"judgement call\" that you will get better at as you gain experience with building complex models. \n",
    "\n",
    "Then answer the graded question and the non-graded question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.D Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1d = Ellipsis\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Which of options 1, 2, or 3 did you choose?\n",
    "\n",
    "a1d = ...                                # Should be integer 1, 2, or 3          \n",
    "\n",
    "print(f'a1d = {a1d}')                    # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.D Non-Graded Answer but Please Do It\n",
    "\n",
    "Describe in a couple of sentences how your examination of the plots led to your decision. (You'll be expected to answer many such questions when you do your project, so this is good practice, and I'll read as many of your answers as I can.)\n",
    "\n",
    "Your answer: \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1.E: Report the Test Score of the Best Model**  \n",
    "Once you have selected the best model in **Part D**, report its **final test score** and answer the graded question.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your best model here on the test data and print out the resulting test RMSE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.E Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1e = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Assign the variable to the test RMSE of the model you selected in Part E\n",
    "\n",
    "a1e = 0.0                                # Just to get it to run without error; your answer here  (be sure to use the RMSE)         \n",
    "\n",
    "print(f'a1e = ${a1e:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two: Random Forests\n",
    "\n",
    "Now you will do the exact same thing as in Problem One, but for `RandomForestRegressor`. \n",
    "(Instructions are omitted, refer to them above.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2.A: Iteratively Sweep Parameters and Visualize Results using `sweep_parameter(...)`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the Random Forest\n",
    "\n",
    "Default_Parameters_Random_Forests = {\n",
    "    'n_estimators': 100,         # Number of base estimators in the ensemble\n",
    "    'max_features': None,        # Number of features to consider when looking for the best split \n",
    "    'max_depth'   : None,        # Limits the depth of each tree\n",
    "    'bootstrap'   : True,        # Use bootstrap samples when building estimators\n",
    "    'random_state': 42,          # Ensures reproducibility\n",
    "    'MSE_found'   : float('inf') # Used for tracking the best MSE during parameter sweeps, to record result of these parameter choices\n",
    "                                 # Initialized to inf in case want to use this to record best MSE found so far in a sequence of parameter choices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.A Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part 2.A\n",
    "\n",
    "a2a = 0.0                                # Just to get it to run without error; your answer here (remember to use the RMSE)         \n",
    "\n",
    "print(f'a2a = ${a2a:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2.B: Refine Parameters for Model Stability**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.B Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part 2.B (it may not be different than Part 2.A)\n",
    "\n",
    "a2b = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a2b = ${a2b:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2.C: Use `GridSearchCV` for Exhaustive Search**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.C Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2c = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found in Part C (it may not be different than Parts A and B)\n",
    "\n",
    "a2c = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a2c = ${a2c:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2.D: Evaluate Model Generalization**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.D Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2d = Ellipsis\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Which of options 1, 2, or 3 did you choose?\n",
    "\n",
    "a2d = ...                                # Should be integer 1, 2, or 3          \n",
    "\n",
    "print(f'a2d = {a2d}')                    # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.D Non-Graded Answer but Please Do It\n",
    "\n",
    "Describe in a couple of sentences how your examination of the plots led to your decision. (You'll be expected to answer many such questions when you do your project, so this is good practice, and I'll read as many of your answers as I can.)\n",
    "\n",
    "Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2.E: Report the Test Score of the Best Model**  \n",
    "Once you have selected the best model, **report its final test score** and answer the graded question.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your best model here on the test data and print out the resulting test RMSE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.E Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2e = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Assign the variable to the test RMSE of the model you selected in Part E\n",
    "\n",
    "a2e = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a2e = ${a2e:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three: Gradient Boosting Trees\n",
    "\n",
    "Now you will do the exact same thing as in Problem One, but for `GradientBoostingRegressor`. \n",
    "(Instructions are omitted, refer to them above.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3.A: Iteratively Sweep Parameters and Visualize Results using `sweep_parameter(...)`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Default_Parameters_GradientBoosting = {\n",
    "    'learning_rate'           : 0.1,             # Shrinks the contribution of each tree. Affects the speed of learning and overfitting.\n",
    "    'n_estimators'            : 100,             # The number of boosting stages to be run. More estimators can improve performance but increase training time.\n",
    "    'max_depth'               : 3,               # Maximum depth of individual trees. Controls model complexity.\n",
    "    'max_features'            : None,            # Number of features to consider when looking for best split. Can help reduce overfitting.\n",
    "    'random_state'            : 42,              # Controls randomness of boosting. Useful for reproducibility.\n",
    "    'MSE_found'               : float('inf')     # NOT a parameter, but will record the MSE found for the current parameter choices\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.A Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3a = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part 3.A\n",
    "\n",
    "a3a = 0.0                                # Just to get it to run without error; your answer here (remember to use the RMSE)         \n",
    "\n",
    "print(f'a3a = ${a3a:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3.B: Refine Parameters for Model Stability**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.B Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found after Part 3.B (it may not be different than Part 3.A)\n",
    "\n",
    "a3b = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a3b = ${a3b:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3.C: Use `GridSearchCV` for Exhaustive Search**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.C Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3c = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Set the variable to best CV RMSE score found in Part C (it may not be different than Parts A and B)\n",
    "\n",
    "a3c = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a3c = ${a3c:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3.D: Evaluate Model Generalization**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.D Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3d = Ellipsis\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Which of options 1, 2, or 3 did you choose?\n",
    "\n",
    "a3d = ...                                # Should be integer 1, 2, or 3          \n",
    "\n",
    "print(f'a3d = {a3d}')                    # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.D Non-Graded Answer but Please Do It\n",
    "\n",
    "Describe in a couple of sentences how your examination of the plots led to your decision. (You'll be expected to answer many such questions when you do your project, so this is good practice, and I'll read as many of your answers as I can.)\n",
    "\n",
    "Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3.E: Report the Test Score of the Best Model**  \n",
    "Once you have selected the best model, **report its final test score** and answer the graded question.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your best model here on the test data and print out the resulting test RMSE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.E Graded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3e = $0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Assign the variable to the test RMSE of the model you selected in Part E\n",
    "\n",
    "a3e = 0.0                                # Just to get it to run without error; your answer here           \n",
    "\n",
    "print(f'a3e = ${a3e:,.2f}')              # Do not change this line, and DO NOT print anything else in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Which `BaggingRegressor` parameters are most important?\n",
    "\n",
    "When exploring ensemble methods like `BaggingRegressor`, it's best to focus at first on the parameters that directly influence both the behavior of the individual base estimators and the overall ensemble performance. Here is an approximate ordering of the parameters from most significant to least. In this homework, we will experiment with the top **four** parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Parameters**\n",
    "\n",
    "1. **n_estimators** (default: **10**)  \n",
    "   *Determines the number of base estimators in the ensemble. Increasing this number can reduce variance and improve performance, though it comes with higher computational cost.*\n",
    "\n",
    "2. **max_samples** (default: **1.0**)  \n",
    "   *Specifies the number (or fraction) of samples to draw from the training set for each base estimator. This is crucial for controlling the diversity of the estimators and can directly affect bias and variance.*\n",
    "\n",
    "3. **max_features** (default: **1.0**)  \n",
    "   *Specifies the number (or fraction) of features to consider when training each base estimator. Adjusting this parameter can help manage overfitting by limiting the complexity of each individual estimator.*\n",
    "\n",
    "4. **bootstrap** (default: **True**)  \n",
    "   *Indicates whether samples are drawn with replacement. Bootstrap sampling introduces randomness into the training process, leading to more diverse estimators and often improved ensemble performance.*\n",
    "\n",
    "---\n",
    "\n",
    "**Less Important Parameters**\n",
    "\n",
    "5. **base_estimator** (default: **None**)  \n",
    "   *Defines the underlying estimator to be used. If set to `None`, BaggingRegressor defaults to using a DecisionTreeRegressor. Experimenting with different base estimators can provide valuable insights into model performance.*\n",
    "\n",
    "6. **oob_score** (default: **False**)  \n",
    "   *If enabled, the model uses out-of-bag samples to estimate the generalization error, providing an internal validation metric without the need for a separate validation set.*\n",
    "\n",
    "7. **bootstrap_features** (default: **False**)  \n",
    "   *Specifies whether features are sampled with replacement. This additional layer of randomness can further increase estimator diversity, though its impact is typically less significant than sample bootstrapping.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2: Which `RandomForestRegressor` parameters are most important?\n",
    "\n",
    "We will focus on the top **four** parameters in this list for `RandomForestRegressor`. \n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Parameters**\n",
    "\n",
    "1. **n_estimators** (default: **100**)  \n",
    "   *Determines the number of trees in the forest. Increasing this number generally improves performance and model stability, albeit with higher computational cost.*\n",
    "\n",
    "2. **max_features** (default: **None**)  \n",
    "   *Specifies the number of features to consider when looking for the best split. Adjusting this can help manage the bias-variance trade-off and affect the diversity among the trees.*\n",
    "\n",
    "3. **max_depth** (default: **None**)  \n",
    "   *Limits the depth of each tree. Restricting the maximum depth is an effective way to control overfitting and reduce the complexity of the model.*\n",
    "\n",
    "4. **bootstrap** (default: **True**)  \n",
    "   *Indicates whether bootstrap samples are used when building trees. Enabling bootstrap sampling introduces randomness into the training process, which can improve the generalization of the ensemble.*\n",
    "\n",
    "---\n",
    "\n",
    "**Less Important Parameters**\n",
    "\n",
    "5. **min_samples_split** (default: **2**)  \n",
    "   *Defines the minimum number of samples required to split an internal node. Tuning this parameter affects how the tree grows, influencing its granularity and robustness.*\n",
    "\n",
    "6. **min_samples_leaf** (default: **1**)  \n",
    "   *Specifies the minimum number of samples that must be present in a leaf node. This parameter ensures that leaves are not created with too few samples, which can help mitigate overfitting.*\n",
    "\n",
    "7. **oob_score** (default: **False**)  \n",
    "   *If enabled, uses out-of-bag samples to estimate the generalization error, providing an internal validation measure without the need for a separate validation set.*\n",
    "\n",
    "8. **criterion** (default: **'squared_error'**)  \n",
    "   *Determines the function used to measure the quality of a split. While its effect is typically subtle, experimenting with this parameter can reveal how different error metrics impact performance.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Which `GradientBoostingRegressor` parameters are most important?\n",
    "\n",
    "We will focus on the top **four** parameters in this list for `GradientBoostingRegressor`. \n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Parameters**\n",
    "\n",
    "1. **learning_rate** (default: **0.1**)  \n",
    "   *Controls the contribution of each individual tree. A lower learning rate generally requires more trees but can lead to improved generalization.*\n",
    "\n",
    "2. **n_estimators** (default: **100**)  \n",
    "   *Specifies the number of boosting stages (i.e., the number of trees in the ensemble). More estimators can improve performance but also increase the risk of overfitting if not tuned properly.*\n",
    "\n",
    "3. **max_depth** (default: **3**)  \n",
    "   *Limits the depth of the individual regression trees. Restricting the depth helps control overfitting and reduces the complexity of each base learner.*\n",
    "\n",
    "4. **max_features** (default: **None**)  \n",
    "   *Controls the number of features to consider when looking for the best split. Adjusting this can influence the bias-variance trade-off of the model.*\n",
    "   \n",
    "---\n",
    "\n",
    "**Less Important Parameters**\n",
    "\n",
    "5. **min_samples_split** (default: **2**)  \n",
    "   *Defines the minimum number of samples required to split an internal node. This parameter controls the growth of each tree and can prevent overly specific splits.*\n",
    "\n",
    "6. **min_samples_leaf** (default: **1**)  \n",
    "   *Specifies the minimum number of samples that must be present in a leaf node. This helps in ensuring that trees do not become too tailored to the training data.*\n",
    "\n",
    "7. **max_leaf_nodes** (default: **None**)  \n",
    "    *An optional parameter that sets a maximum number of leaf nodes for each tree. This can provide an additional way to control the complexity of the model.*\n",
    "\n",
    "8. **subsample** (default: **1.0**)  \n",
    "   *Determines the fraction of samples used for fitting each individual tree. Values less than 1.0 introduce randomness into the boosting process, which can help reduce overfitting.*\n",
    "\n",
    "9. **loss** (default: **'squared_error'**)  \n",
    "   *Determines the loss function to be optimized during training. Different loss functions can be used depending on the specific characteristics of the regression problem.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 4: Tips on Tuning Complicated Models\n",
    "\n",
    "### Using `sweep_parameters` for Single-Parameter Exploration\n",
    "\n",
    "1. **Purpose**  \n",
    "   - The function `sweep_parameters` automates the process of iterating over a list of parameter values (e.g., `n_estimators` in a random forest) and training a model for each value.\n",
    "   - It then computes and plots, for each value in the specified range:\n",
    "     - **Training MSE**: How well the model fits the training data.  \n",
    "     - **Cross-Validation (CV) MSE**: An estimate of how well the model generalizes, averaged over multiple folds (and possibly multiple repeats).  \n",
    "     - **Test MSE**: If you have a dedicated test set, this provides a final check of out-of-sample performance.  \n",
    "     - **Std of CV Scores**: The standard deviation across cross-validation folds (and repeats), indicating how stable or variable the model performance is.\n",
    "\n",
    "2. **Interpretation of the Plots**  \n",
    "   - **Training MSE vs. Parameter Value**:  \n",
    "     Helps you see when the model is overfitting (training MSE much lower than CV MSE) or underfitting (training MSE is high).\n",
    "   - **CV MSE vs. Parameter Value**:  \n",
    "     Typically your key metric for choosing the parameter setting. Look for a valley in this curve.\n",
    "   - **Test MSE vs. Parameter Value**:  \n",
    "     The precise values here are dependent on the random split of training and testing sets; we include it so that we may examine the **gap** between CV MSE and test MSE (see Appendix 5). \n",
    "   - **Std of CV Scores vs. Parameter Value**:  \n",
    "     Large standard deviation means the model’s performance is inconsistent across folds (and repeats). You may want to pick a parameter setting that not only has a low mean CV MSE but also a lower standard deviation for more reliable performance.\n",
    "   - **Beware of the Scale of the Plots!**\n",
    "     The plots are drawn to fit the values, and the scale of the y-axis may vary quite a lot!  Therefore, you can not just \"eyeball\" a curve and make quick decisions that, say, the std of the CV score indicates something important. **It depends on the scale of the y-axis.**  If the curve(s) are relatively flat, the differences shown may in reality be quite small!  Just be careful to observe both the shape and the scale of the plots. \n",
    "3. **Sweeping Strategy**  \n",
    "   - **Coarse to Fine**: Start with wide ranges and larger steps (e.g., 100–1000 in increments of 100). Narrow down once you see the region where the CV MSE is lowest. Then reduce step sizes in that region (e.g., steps of 25 or 10).\n",
    "   - **Stop Early if Converged**: If the CV MSE difference between 1400 and 1405 `n_estimators` is negligible, further fine-tuning is unlikely to yield real improvement.\n",
    "\n",
    "**Note:** We will not be using early stopping in this homework, because we are still learning how to interpret model performance, but with further experience, you may find that early stopping will help you zero in on the best models. \n",
    "\n",
    "---\n",
    "\n",
    "### Adjusting Repeated Cross-Validation\n",
    "\n",
    "1. **Why Repeat?**  \n",
    "   - Cross-validation is already a good measure of generalization. However, it’s still sensitive to how the data is split into folds. Repeating cross-validation with different random folds provides a more robust estimate (reducing the variance of the CV score).\n",
    "\n",
    "2. **When to Increase Repeats**  \n",
    "   - **Early Sweeps**: Use 1 or 2 repeats when you are scanning large ranges of your parameter. The goal is speed and a rough idea of where the “sweet spot” is, so you don’t want to spend too much computation time on many repeats.\n",
    "   - **Narrowed Sweeps**: Once you have focused on a smaller interval (e.g., 1300–1500 in increments of 25 for `n_estimators`), increase repeats to 5 (ok) or 10 (ideal). This gives you a more stable estimate and helps confirm the final optimal setting with greater confidence.\n",
    "\n",
    "3. **Trade-Offs**  \n",
    "   - **Computation**: Each extra repeat multiplies the total training time. If you have limited computational resources, keep the repeats modest until you are zeroing in on a smaller range.\n",
    "   - **Stability**: More repeats lower the variance of your CV estimate. If you see widely fluctuating performance across folds, use more repeats to gauge if it’s truly unstable or just a byproduct of the data splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Tips for a Smooth Workflow\n",
    "\n",
    "1. **Plot and Observe**  \n",
    "   Always visualize the training/CV/test MSE vs. parameter and the CV std plots. Look for patterns of overfitting or underfitting, and watch the standard deviation for instability.\n",
    "\n",
    "2. **Document Iterations using Parameter Dictionaries**  \n",
    "   In the video notebook for Week 7 on Gradient Boosting, we stored the parameter dictionaries in a list; you may wish to do something similar, or find some other way of recording your results. \n",
    "\n",
    "3. **Scale Up Gradually**  \n",
    "   Start simply (fewer repeats, coarse parameter steps), refine as you learn more, and only then use heavier computations (more repeats, fine steps).\n",
    "\n",
    "4. **Use `GridSearchCV` Judiciously**  \n",
    "   Grid and random search are powerful tools, but can lead to hours of computation--and without a progress bar to tell you how far you've gotten! Use these tools to refine and verify your search after you understand the search space through plotting and visualization. The same \"scale up gradually\" approach should be used here as well. \n",
    "\n",
    "5. **Take Advantage of Parallelism with `n_jobs=-1`**  \n",
    "   Many scikit-learn functions, such as cross-validation and grid search routines, allow you to specify the `n_jobs` parameter. We have set the default at `n_jobs=-1` to use all available CPU cores to speed up processing.  \n",
    "   - **If You See Warnings**: Sometimes, you may get a message like “A worker stopped while some jobs were still running.” This can happen if your system is running out of memory or hitting other resource limits. Mostly you can ignore these.  But if you are concerned, reduce the number of jobs (e.g., `n_jobs=4`) to reduce the load.  You can also track the system load using the Activity Monitor (on Mac) or Task Manager (on PC). \n",
    "   - **Utilize a Robust Environment**: If you need more parallel processing power, consider using a computing environment such as Google Colab, which offers additional CPU cores to handle more intensive jobs efficiently.\n",
    "\n",
    "By combining the coarse-to-fine parameter sweeping with increasing repeats as you zero in on the most promising configurations, you balance computational efficiency with the need for robust, stable model performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 5:  Model Selection:  When is a model overfitting? Which model to choose?\n",
    "\n",
    "- **Minimize Mean CV MSE:** Aim for the model configuration with the lowest mean CV MSE as a primary indicator of good generalization.\n",
    "- **Stable CV Performance:** Choose a model where the CV MSE either\n",
    "  - Reaches a minimum and then starts to increase, or\n",
    "  - Plateaus—indicating additional improvements are marginal—rather than a continuously decreasing trend that may signal overfitting.\n",
    "- **Gap Analysis:** Watch out for an *increasing gap* between the mean CV MSE and testing MSE. A growing gap can be a sign of overfitting to the training data. Do not worry about the *size* of the gap, which will primarily be due to the random split between training and testing sets; it is the *change* in the gap between the two that is most significant. \n",
    "- **CV Score Consistency:** Favor models with a lower standard deviation in CV scores, as this reflects more consistent performance across different data splits.\n",
    "- **Be Aware of the Scale:** You need to look not only at the shape of the curves, but the scale of the y-axis, particularly with the std of the CV scores.  Beware of assuming that there is a significant difference unless you take account of the actual values!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_env] *",
   "language": "python",
   "name": "conda-env-ml_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
